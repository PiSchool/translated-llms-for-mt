peft_mode : True # True is fine-tuning with LoRA, False to fine-tune without PEFT
max_length : 64
lr : 3e-4
num_epochs : 4
batch_size : 1
accumulate_grad_num : 4
lora_alpha : 32
lora_dropout : 0.1
lora_r : 16
# where you want to save/load the lora module
lora_path : models/t5-small_peft_translated_it_en/